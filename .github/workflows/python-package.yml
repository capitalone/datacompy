# This workflow will install Python dependencies, run tests and lint with a variety of Python versions
# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions

name: Python package

on:
  push:
    branches: [develop, master]
  pull_request:
    branches: [develop, master]

jobs:
  build:

    runs-on: ubuntu-latest 
    strategy:
      fail-fast: false
      matrix:
        python-version: [3.6, 3.7, 3.8, 3.9]
        spark: [2.4.3, 2.4.4, 2.4.5]
        exclude:
          - python-version: 3.8
            spark: 2.4.3
          - python-version: 3.8
            spark: 2.4.4
          - python-version: 3.8
            spark: 2.4.5
          - python-version: 3.9
            spark: 2.4.3
          - python-version: 3.9
            spark: 2.4.4
          - python-version: 3.9
            spark: 2.4.5
    env:
      PYTHON_VERSION: ${{ matrix.python-version }} 
      SPARK_VERSION: ${{ matrix.spark }}

    steps:
    - uses: actions/checkout@v2
      
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v2
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Setup Java JDK
      uses: actions/setup-java@v1.4.3 
      with:
        java-version: 1.8
        
    - name: Install Spark (2.4.x)
      if: matrix.spark == '2.4.3' || matrix.spark == '2.4.4' || matrix.spark == '2.4.5'
      run: |
        wget -q -O spark.tgz https://archive.apache.org/dist/spark/spark-${{ matrix.spark }}/spark-${{ matrix.spark }}-bin-hadoop2.7.tgz
        tar xzf spark.tgz
        rm spark.tgz
        echo "SPARK_HOME=${{ runner.workspace }}/datacompy/spark-${{ matrix.spark }}-bin-hadoop2.7" >> $GITHUB_ENV
        echo "${{ runner.workspace }}/datacompy/spark-${{ matrix.spark }}-bin-hadoop2.7/bin" >> $GITHUB_PATH

    - name: Install Spark (3.0.x)
      if: matrix.spark == '3.0.0' || matrix.spark == '3.0.1'
      run: |
        wget -q -O spark.tgz https://archive.apache.org/dist/spark/spark-${{ matrix.spark }}/spark-${{ matrix.spark }}-bin-hadoop3.2.tgz
        tar xzf spark.tgz
        rm spark.tgz
        echo "SPARK_HOME=${{ runner.workspace }}/datacompy/spark-${{ matrix.spark }}-bin-hadoop3.2" >> $GITHUB_ENV
        echo "${{ runner.workspace }}/datacompy/spark-${{ matrix.spark }}-bin-hadoop3.2/bin" >> $GITHUB_PATH
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        python -m pip install pytest pytest-spark pypandoc
        python -m pip install pyspark==${{ matrix.spark }}
        if [ -f test-requirements.txt ]; then pip install -r test-requirements.txt; fi
    
    - name: Test with pytest
      run: |
        python -m pytest tests/
